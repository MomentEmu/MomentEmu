{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MomentEmu Documentation","text":"<p>Welcome to the official documentation for MomentEmu, a lightweight, interpretable polynomial emulator for smooth mappings with auto-differentiation support.</p>"},{"location":"#what-is-momentemu","title":"What is MomentEmu?","text":"<p>MomentEmu implements the moment-projection polynomial emulator introduced in Zhang (2025) (arXiv:2507.02179). It builds interpretable, closed-form polynomial emulators via moment matrices, achieving millisecond-level inference and symbolic transparency.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>\u2728 Pure Python implementation with minimal dependencies</li> <li>\ud83d\ude80 Fast training via moment matrices; near-instant inference</li> <li>\ud83d\udd04 Bidirectional emulation: forward (\u03b8 \u2192 y) and inverse (y \u2192 \u03b8)</li> <li>\ud83e\uddee Auto-differentiation support via JAX, PyTorch, and SymPy</li> <li>\ud83d\udcca Symbolic expressions for full interpretability</li> <li>\ud83c\udfaf Suitable for MCMC, Bayesian inference, sensitivity analyses</li> <li>\ud83d\udce6 Compact - no heavy model files</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>from MomentEmu import PolyEmu\n\n# Create emulator\nemulator = PolyEmu(X_train, Y_train, forward=True, backward=True)\n\n# Forward prediction: parameters \u2192 observables\nY_pred = emulator.forward_emulator(X_new)\n\n# Inverse estimation: observables \u2192 parameters\nX_est = emulator.backward_emulator(Y_new)\n\n# Get symbolic expressions\nforward_expr = emulator.generate_forward_symb_emu()\n</code></pre>"},{"location":"#auto-differentiation-frameworks","title":"Auto-Differentiation Frameworks","text":"<p>MomentEmu supports three auto-differentiation frameworks:</p> JAXPyTorchSymPy <p>High-performance computing with JIT compilation <pre><code>from jax_momentemu import create_jax_emulator\njax_emu = create_jax_emulator(emulator)\n</code></pre></p> <p>Neural network integration and ML pipelines <pre><code>from torch_momentemu import create_torch_emulator\ntorch_emu = create_torch_emulator(emulator)\n</code></pre></p> <p>Exact symbolic differentiation <pre><code>from symbolic_momentemu import create_symbolic_emulator\nsym_emu = create_symbolic_emulator(emulator)\n</code></pre></p>"},{"location":"#navigation","title":"Navigation","text":"<p>Use the navigation menu to explore:</p> <ul> <li>Installation - Get started with MomentEmu</li> <li>Tutorials - Step-by-step guides</li> <li>API Reference - Detailed function documentation</li> <li>Theory - Mathematical foundations</li> <li>Examples - Real-world applications</li> </ul>"},{"location":"#external-resources","title":"External Resources","text":"<ul> <li>\ud83d\udcc4 Paper: arXiv:2507.02179</li> <li>\ud83e\uddea Examples: MomentEmu-PolyCAMB-examples</li> <li>\ud83d\udc1b Issues: GitHub Issues</li> <li>\ud83d\udcac Discussions: GitHub Discussions</li> </ul>"},{"location":"#how-it-works","title":"How It Works","text":"<p>MomentEmu builds polynomial emulators by solving a linear system:</p> <ul> <li>Moment matrix: \\(M_{\\alpha\\beta} = \\frac{1}{N} \\sum_{i} \\theta_i^\\alpha \\theta_i^\\beta\\)</li> <li>Moment vector: \\(\\nu_\\alpha = \\frac{1}{N} \\sum_{i} \\theta_i^\\alpha y_i\\)</li> <li>Solution: \\(M c = \\nu\\) finds polynomial coefficients \\(c\\)</li> </ul> <p>No iterative optimization needed! Model selection uses validation RMSE.</p>"},{"location":"api/core/","title":"Core API Reference","text":"<p>This page documents the core MomentEmu API for polynomial emulation.</p>"},{"location":"api/core/#polyemu-class","title":"PolyEmu Class","text":"Source code in <code>src/MomentEmu/MomentEmu.py</code> <pre><code>class PolyEmu():\n    def __init__(self, \n                X, \n                Y, \n                X_test=None, \n                Y_test=None, \n                test_size=0.2, \n                RMSE_tol=1e-2, \n                forward=True, \n                backward=False,\n                init_deg_forward=None, \n                max_degree_forward=10, \n                init_deg_backward=None, \n                max_degree_backward=10, \n                return_max_frac_err=False):\n        self.n_params = X.shape[1]\n        self.n_outputs = Y.shape[1]\n        if X_test is None or Y_test is None:\n            # Split into training and validation\n            X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=test_size)\n        else:\n            X_train, Y_train = X, Y\n            X_val, Y_val = X_test, Y_test\n\n        # Scale the training data\n        self.scaler_X = StandardScaler()\n        self.scaler_Y = StandardScaler()\n\n        X_train_scaled = self.scaler_X.fit_transform(X_train)\n        Y_train_scaled = self.scaler_Y.fit_transform(Y_train)\n        X_val_scaled = self.scaler_X.transform(X_val)\n        Y_val_scaled = self.scaler_Y.transform(Y_val)\n\n        if forward:\n            print(\"Generating forward emulator...\")\n            self.generate_forward_emulator(X_train_scaled, \n                                        Y_train_scaled,\n                                        X_val_scaled,\n                                        Y_val_scaled,\n                                        RMSE_tol=RMSE_tol, \n                                        init_deg=init_deg_forward, \n                                        max_degree=max_degree_forward)\n            if return_max_frac_err:\n                Y_val_pred = self.forward_emulator(X_val)\n                max_frac_err = np.max(np.abs((Y_val_pred - Y_val) / (Y_val+1e-10)))\n                self.forward_max_frac_err = max_frac_err\n                print(f\"Forward emulator maximum fractional error: {max_frac_err}. (If the true value is close to 0, this value could be extremely large. This is fine.)\")\n\n        if backward:\n            print(\"Generating backward emulator...\")\n            self.generate_backward_emulator(X_train_scaled, \n                                            Y_train_scaled,\n                                            X_val_scaled,\n                                            Y_val_scaled,\n                                            RMSE_tol=RMSE_tol, \n                                            init_deg=init_deg_backward, \n                                            max_degree=max_degree_backward)\n            if return_max_frac_err:\n                X_val_pred = self.backward_emulator(Y_val)\n                max_frac_err = np.max(np.abs((X_val_pred - X_val) / (X_val+1e-10)))\n                self.backward_max_frac_err = max_frac_err\n                print(f\"Backward emulator maximum fractional error: {max_frac_err}. (If the true value is close to 0, this value could be extremely large. This is fine.)\")\n\n    def generate_forward_emulator(self, \n                                  X_train_scaled, \n                                  Y_train_scaled,\n                                  X_val_scaled,\n                                  Y_val_scaled,\n                                  RMSE_tol=1e-3, \n                                  init_deg=None, \n                                  max_degree=10):\n\n        if init_deg is None:\n            if self.n_params &gt; 6:\n                degree = 1\n            elif self.n_params &lt; 3:\n                degree = 3\n            else:\n                degree = 2\n        else:\n            degree = init_deg\n\n        assert degree &lt;= max_degree, \"Initial degree must be less than or equal to max_degree\"\n\n        coeffs_list = []\n        RMSE_val_list = []\n        multi_indices_list = []\n\n        for d in range(degree, max_degree + 1):\n            M, nu, multi_indices = compute_moments_vector_output(X_train_scaled, Y_train_scaled, d)\n            coeffs = solve_emulator_coefficients(M, nu)\n\n            Y_val_pred = evaluate_emulator(X_val_scaled, coeffs, multi_indices)\n\n            RMSE_val = np.sqrt(mean_squared_error(Y_val_scaled, Y_val_pred))\n\n            coeffs_list.append(coeffs)\n            multi_indices_list.append(multi_indices)\n            RMSE_val_list.append(RMSE_val)\n\n            if RMSE_val &lt; RMSE_tol:\n                self.foward_degree = d\n                print(f\"Forward emulator generated with degree {d}, RMSE_val of {RMSE_val}.\")\n                break\n            if d == max_degree:\n                # find the degree with the lowest RMSE\n                ind = np.argmin(RMSE_val_list)\n                coeffs = coeffs_list[ind]\n                multi_indices = multi_indices_list[ind]\n                self.foward_degree = degree + ind\n                warning(f\"Maximum degree {max_degree} reached. Returning emulator with degree {degree+ind} with RMSE_val of {RMSE_val_list[ind]}.\")\n\n        self.forward_coeffs = coeffs\n        self.forward_multi_indices = multi_indices\n\n        pass\n\n    def forward_emulator(self, X):\n        if X.ndim == 1:\n            X = X.reshape(1, -1)\n        X_scaled = self.scaler_X.transform(X)\n        Y_pred_scaled = evaluate_emulator(X_scaled, self.forward_coeffs, self.forward_multi_indices)\n        Y_pred = self.scaler_Y.inverse_transform(Y_pred_scaled)\n        return Y_pred\n\n    def generate_backward_emulator(self, \n                                   X_train_scaled, \n                                   Y_train_scaled,\n                                   X_val_scaled,\n                                   Y_val_scaled,\n                                   RMSE_tol=1e-2, \n                                   init_deg=None, \n                                   max_degree=10):\n        if init_deg is None:\n            if self.n_outputs &gt; 6:\n                degree = 1\n            elif self.n_outputs &lt; 3:\n                degree = 3\n            else:\n                degree = 2\n        else:\n            degree = init_deg\n\n        assert degree &lt;= max_degree, \"Initial degree must be less than or equal to max_degree\"\n\n        coeffs_list = []\n        RMSE_val_list = []\n        multi_indices_list = []\n\n        for d in range(degree, max_degree + 1):\n            M, nu, multi_indices = compute_moments_vector_output(Y_train_scaled, X_train_scaled, d)\n            coeffs = solve_emulator_coefficients(M, nu)\n\n            X_val_pred = evaluate_emulator(Y_val_scaled, coeffs, multi_indices)\n\n            RMSE_val = np.sqrt(mean_squared_error(X_val_scaled, X_val_pred))\n\n            coeffs_list.append(coeffs)\n            multi_indices_list.append(multi_indices)\n            RMSE_val_list.append(RMSE_val)\n\n            if RMSE_val &lt; RMSE_tol:\n                self.backward_degree = d\n                print(f\"Backward emulator generated with degree {d}, RMSE_val of {RMSE_val}.\")\n                break\n            if d == max_degree:\n                # find the degree with the lowest RMSE\n                ind = np.argmin(RMSE_val_list)\n                coeffs = coeffs_list[ind]\n                multi_indices = multi_indices_list[ind]\n                self.backward_degree = degree + ind\n                warning(f\"Maximum degree {max_degree} reached. Returning emulator with degree {degree+ind} with RMSE_val of {RMSE_val_list[ind]}.\")\n\n        self.backward_coeffs = coeffs\n        self.backward_multi_indices = multi_indices\n\n        pass\n\n    def backward_emulator(self, Y):\n        if Y.ndim == 1:\n            Y = Y.reshape(1, -1)\n        Y_scaled = self.scaler_Y.transform(Y)\n        X_pred_scaled = evaluate_emulator(Y_scaled, self.backward_coeffs, self.backward_multi_indices)\n        X_pred = self.scaler_X.inverse_transform(X_pred_scaled)\n        return X_pred\n\n    def generate_forward_symb_emu(self, variable_names=None):\n        exprs = symbolic_polynomial_expressions(self.forward_coeffs, \n                                                self.forward_multi_indices, \n                                                variable_names=variable_names, \n                                                input_means=self.scaler_X.mean_, \n                                                input_vars=self.scaler_X.var_,\n                                                output_means=self.scaler_Y.mean_, \n                                                output_vars=self.scaler_Y.var_)\n        return exprs\n\n    def generate_backward_symb_emu(self, variable_names=None):\n        exprs = symbolic_polynomial_expressions(self.backward_coeffs, \n                                                self.backward_multi_indices, \n                                                variable_names=variable_names, \n                                                input_means=self.scaler_Y.mean_, \n                                                input_vars=self.scaler_Y.var_,\n                                                output_means=self.scaler_X.mean_,\n                                                output_vars=self.scaler_X.var_)\n        return exprs\n</code></pre>"},{"location":"api/core/#utility-functions","title":"Utility Functions","text":""},{"location":"api/core/#evaluate_emulator","title":"evaluate_emulator","text":"<p>Evaluate the polynomial emulator at inputs X using known coefficients. X: N x n coeffs: D x m multi_indices: list of \u03b1 Returns: Y_pred: N x m</p> Source code in <code>src/MomentEmu/MomentEmu.py</code> <pre><code>def evaluate_emulator(X, coeffs, multi_indices):\n    \"\"\"\n    Evaluate the polynomial emulator at inputs X using known coefficients.\n    X: N x n\n    coeffs: D x m\n    multi_indices: list of \u03b1\n    Returns: Y_pred: N x m\n    \"\"\"\n    Phi = evaluate_monomials_lazy(X, multi_indices)  # N x D\n    return Phi @ coeffs  # N x m\n</code></pre>"},{"location":"api/core/#symbolic_polynomial_expressions","title":"symbolic_polynomial_expressions","text":"<p>Convert emulator coefficients into sympy expressions. coeffs: D x m (number of basis terms \u00d7 number of outputs) multi_indices: list of \u03b1 Returns: list of sympy expressions, one per output dimension</p> Source code in <code>src/MomentEmu/MomentEmu.py</code> <pre><code>def symbolic_polynomial_expressions(coeffs, multi_indices, variable_names=None, \n                                    input_means=None, input_vars=None, \n                                    output_means=None, output_vars=None):\n    \"\"\"\n    Convert emulator coefficients into sympy expressions.\n    coeffs: D x m (number of basis terms \u00d7 number of outputs)\n    multi_indices: list of \u03b1\n    Returns: list of sympy expressions, one per output dimension\n    \"\"\"\n    D, m = coeffs.shape\n    n = len(multi_indices[0])\n    if variable_names is None:\n        variable_names = [f\"x{i+1}\" for i in range(n)]\n    vars_sym = sp.symbols(variable_names)\n\n    if input_vars is not None:\n        input_stds = np.sqrt(input_vars)\n    else:\n        input_stds = None\n    if output_vars is not None:\n        output_stds = np.sqrt(output_vars)\n    else:\n        output_stds = None\n\n    expressions = []\n    for j in range(m):  # For each output dimension\n        expr = 0\n        for c, alpha in zip(coeffs[:, j], multi_indices):\n            if input_means is not None and input_stds is not None:\n                monomial = np.prod([ ( (vars_sym[i] - input_means[i]) / input_stds[i] )**alpha[i] for i in range(n)])\n            elif input_means is not None:\n                monomial = np.prod([ (vars_sym[i] - input_means[i])**alpha[i] for i in range(n)])\n            expr += c * monomial\n        if output_means is not None and output_stds is not None:\n            expr = expr * output_stds[j] + output_means[j]\n        elif output_means is not None:\n            expr = expr + output_means[j]\n        expressions.append(sp.simplify(expr))\n    return expressions\n</code></pre>"},{"location":"api/core/#low-level-functions","title":"Low-Level Functions","text":""},{"location":"api/core/#generate_multi_indices","title":"generate_multi_indices","text":"<p>Generate all multi-indices \u03b1 in \u2115^n with total degree \u2264 d.</p> Source code in <code>src/MomentEmu/MomentEmu.py</code> <pre><code>def generate_multi_indices(n, d):\n    \"\"\"Generate all multi-indices \u03b1 in \u2115^n with total degree \u2264 d.\"\"\"\n    indices = []\n    for deg in range(d + 1):\n        for c in combinations_with_replacement(range(n), deg):\n            counter = Counter(c)\n            alpha = [counter[i] for i in range(n)]\n            indices.append(tuple(alpha))\n    return indices\n</code></pre>"},{"location":"api/core/#evaluate_monomials","title":"evaluate_monomials","text":"<p>Evaluate \u03c6_\u03b1(X) for all samples and all \u03b1.</p> Source code in <code>src/MomentEmu/MomentEmu.py</code> <pre><code>def evaluate_monomials(X, multi_indices):\n    \"\"\"Evaluate \u03c6_\u03b1(X) for all samples and all \u03b1.\"\"\"\n    N, n = X.shape\n    D = len(multi_indices)\n    Phi = np.empty((N, D))\n    for j, alpha in enumerate(multi_indices):\n        Phi[:, j] = np.prod(X ** alpha, axis=1)\n    return Phi  # shape: N x D\n</code></pre>"},{"location":"api/core/#compute_moments_vector_output","title":"compute_moments_vector_output","text":"<p>Vector-valued version of moment method. X: N x n input parameter array Y: N x m observable array degree: total degree of monomials Returns: moment matrix M, moment vectors \u03bd (D x m), and multi-indices</p> Source code in <code>src/MomentEmu/MomentEmu.py</code> <pre><code>def compute_moments_vector_output(X, Y, degree):\n    \"\"\"\n    Vector-valued version of moment method.\n    X: N x n input parameter array\n    Y: N x m observable array\n    degree: total degree of monomials\n    Returns: moment matrix M, moment vectors \u03bd (D x m), and multi-indices\n    \"\"\"\n    N, n = X.shape\n    m = Y.shape[1]  # number of outputs\n\n    multi_indices = generate_multi_indices(n, degree)\n    D = len(multi_indices)\n    Phi = evaluate_monomials_lazy(X, multi_indices)  # N x D\n\n    M = (Phi.T @ Phi) / N                       # D x D\n    nu = (Phi.T @ Y) / N                        # D x m\n\n    return M, nu, multi_indices\n</code></pre>"},{"location":"api/core/#solve_emulator_coefficients","title":"solve_emulator_coefficients","text":"<p>Solve Mc = \u03bd for each output dimension Returns: coefficients array of shape D x m</p> Source code in <code>src/MomentEmu/MomentEmu.py</code> <pre><code>def solve_emulator_coefficients(M, nu):\n    \"\"\"\n    Solve Mc = \u03bd for each output dimension\n    Returns: coefficients array of shape D x m\n    \"\"\"\n    return np.linalg.solve(M, nu)  # D x m\n</code></pre>"},{"location":"api/jax/","title":"JAX Integration API","text":"<p>JAX-based auto-differentiable MomentEmu implementation</p> <p>This module provides JAX integration for MomentEmu, enabling: - High-performance computing with JIT compilation - Automatic differentiation for gradients, Jacobians, and Hessians - GPU acceleration support - Vectorized batch operations</p> <p>Key functions: - create_jax_emulator(): Convert trained MomentEmu to JAX format - demo_jax_autodiff(): Demonstration of JAX auto-differentiation capabilities</p>"},{"location":"api/jax/#MomentEmu.jax_momentemu.create_jax_emulator","title":"<code>create_jax_emulator(emulator)</code>","text":"<p>Convert trained MomentEmu to JAX-differentiable function.</p> Source code in <code>src/MomentEmu/jax_momentemu.py</code> <pre><code>def create_jax_emulator(emulator):\n    \"\"\"Convert trained MomentEmu to JAX-differentiable function.\"\"\"\n\n    # Extract learned parameters\n    coeffs = jnp.array(emulator.forward_coeffs)\n    multi_indices = emulator.forward_multi_indices\n\n    # Extract scaling parameters\n    input_mean = jnp.array(emulator.scaler_X.mean_)\n    input_scale = jnp.array(emulator.scaler_X.scale_)\n    output_mean = jnp.array(emulator.scaler_Y.mean_)\n    output_scale = jnp.array(emulator.scaler_Y.scale_)\n\n    def evaluate_monomials_jax(X_scaled, multi_indices):\n        \"\"\"JAX-compatible monomial evaluation.\"\"\"\n        if X_scaled.ndim == 1:\n            X_scaled = X_scaled.reshape(1, -1)\n\n        N, n = X_scaled.shape\n        D = len(multi_indices)\n\n        Phi = jnp.ones((N, D))\n        for j, alpha in enumerate(multi_indices):\n            monomial = jnp.ones(N)\n            for i, deg in enumerate(alpha):\n                if deg &gt; 0:\n                    monomial = monomial * (X_scaled[:, i] ** deg)\n            Phi = Phi.at[:, j].set(monomial)\n\n        return Phi\n\n    @jax.jit\n    def jax_emulator(X):\n        \"\"\"JAX-compiled differentiable emulator.\"\"\"\n        # Handle single sample\n        if X.ndim == 1:\n            X = X.reshape(1, -1)\n\n        # Scale inputs\n        X_scaled = (X - input_mean) / input_scale\n\n        # Evaluate polynomials\n        Phi = evaluate_monomials_jax(X_scaled, multi_indices)\n\n        # Predict scaled outputs\n        Y_scaled = Phi @ coeffs\n\n        # Unscale outputs\n        Y = Y_scaled * output_scale + output_mean\n\n        return Y\n\n    return jax_emulator\n</code></pre>"},{"location":"api/jax/#MomentEmu.jax_momentemu.demo_jax_autodiff","title":"<code>demo_jax_autodiff()</code>","text":"<p>Demonstrate auto-differentiation with JAX.</p> Source code in <code>src/MomentEmu/jax_momentemu.py</code> <pre><code>def demo_jax_autodiff():\n    \"\"\"Demonstrate auto-differentiation with JAX.\"\"\"\n\n    # Train regular MomentEmu\n    print(\"Training MomentEmu...\")\n    np.random.seed(42)\n    X_train = np.random.uniform(-1, 1, (100, 2))\n    Y_train = (X_train[:, 0]**2 + X_train[:, 1]**2).reshape(-1, 1)\n\n    emulator = PolyEmu(X_train, Y_train, forward=True, backward=False)\n\n    # Convert to JAX\n    print(\"Converting to JAX...\")\n    jax_emu = create_jax_emulator(emulator)\n\n    # Test point\n    x_test = jnp.array([0.5, 0.3])\n\n    # Forward pass\n    y_pred = jax_emu(x_test)\n    print(f\"Prediction: {y_pred}\")\n\n    # Compute gradient\n    grad_fn = grad(lambda x: jax_emu(x).sum())\n    gradient = grad_fn(x_test)\n    print(f\"Gradient: {gradient}\")\n\n    # Compute Jacobian\n    jac_fn = jacfwd(jax_emu)\n    jacobian = jac_fn(x_test)\n    print(f\"Jacobian shape: {jacobian.shape}\")\n    print(f\"Jacobian: {jacobian}\")\n\n    # Compute Hessian\n    hess_fn = jacfwd(jacrev(lambda x: jax_emu(x).sum()))\n    hessian = hess_fn(x_test)\n    print(f\"Hessian: {hessian}\")\n</code></pre>"},{"location":"api/symbolic/","title":"SymPy Integration API","text":"<p>Symbolic auto-differentiation MomentEmu implementation using SymPy</p> <p>This module provides SymPy integration for MomentEmu, enabling: - Exact symbolic differentiation with zero numerical error - Arbitrary-order derivatives (gradients, Hessians, etc.) - Mathematical expression analysis and manipulation - Educational insights into polynomial structure</p> <p>Key components: - SymbolicMomentEmu: SymPy wrapper for symbolic computation - create_symbolic_emulator(): Convert trained MomentEmu to symbolic format - demo_symbolic_autodiff(): Demonstration of symbolic auto-differentiation capabilities</p>"},{"location":"api/symbolic/#MomentEmu.symbolic_momentemu.SymbolicMomentEmu","title":"<code>SymbolicMomentEmu</code>","text":"<p>Symbolic differentiable MomentEmu using SymPy.</p> Source code in <code>src/MomentEmu/symbolic_momentemu.py</code> <pre><code>class SymbolicMomentEmu:\n    \"\"\"Symbolic differentiable MomentEmu using SymPy.\"\"\"\n\n    def __init__(self, trained_emulator, variable_names=None):\n        self.emulator = trained_emulator\n\n        # Get symbolic expressions\n        n_inputs = trained_emulator.n_params\n        if variable_names is None:\n            variable_names = [f'x{i}' for i in range(n_inputs)]\n\n        self.variables = sp.symbols(variable_names)\n        self.expressions = trained_emulator.generate_forward_symb_emu(variable_names)\n\n        # Create lambdified functions for fast numerical evaluation\n        self.lambdified = [sp.lambdify(self.variables, expr, 'numpy') \n                          for expr in self.expressions]\n\n        # Create gradient functions\n        self.gradient_exprs = []\n        self.gradient_funcs = []\n\n        for expr in self.expressions:\n            grad_expr = [sp.diff(expr, var) for var in self.variables]\n            grad_func = [sp.lambdify(self.variables, g_expr, 'numpy') \n                        for g_expr in grad_expr]\n            self.gradient_exprs.append(grad_expr)\n            self.gradient_funcs.append(grad_func)\n\n        # Create Hessian functions\n        self.hessian_exprs = []\n        self.hessian_funcs = []\n\n        for expr in self.expressions:\n            hess_expr = [[sp.diff(expr, var1, var2) for var2 in self.variables] \n                        for var1 in self.variables]\n            hess_func = [[sp.lambdify(self.variables, h_expr, 'numpy') \n                         for h_expr in h_row] for h_row in hess_expr]\n            self.hessian_exprs.append(hess_expr)\n            self.hessian_funcs.append(hess_func)\n\n    def predict(self, X):\n        \"\"\"Evaluate the emulator at X.\"\"\"\n        if X.ndim == 1:\n            X = X.reshape(1, -1)\n\n        predictions = []\n        for i, func in enumerate(self.lambdified):\n            pred = np.array([func(*x) for x in X])\n            predictions.append(pred)\n\n        return np.column_stack(predictions)\n\n    def gradient(self, X):\n        \"\"\"Compute gradient at X.\"\"\"\n        if X.ndim == 1:\n            X = X.reshape(1, -1)\n\n        gradients = []\n        for x in X:\n            grad_at_x = []\n            for output_idx, grad_funcs in enumerate(self.gradient_funcs):\n                grad_output = np.array([func(*x) for func in grad_funcs])\n                grad_at_x.append(grad_output)\n            gradients.append(np.array(grad_at_x))\n\n        return np.array(gradients)\n\n    def hessian(self, X):\n        \"\"\"Compute Hessian at X.\"\"\"\n        if X.ndim == 1:\n            X = X.reshape(1, -1)\n\n        hessians = []\n        for x in X:\n            hess_at_x = []\n            for output_idx, hess_funcs in enumerate(self.hessian_funcs):\n                hess_matrix = np.array([[func(*x) for func in row] for row in hess_funcs])\n                hess_at_x.append(hess_matrix)\n            hessians.append(np.array(hess_at_x))\n\n        return np.array(hessians)\n\n    def get_symbolic_expressions(self):\n        \"\"\"Get the symbolic expressions.\"\"\"\n        return {\n            'expressions': self.expressions,\n            'gradients': self.gradient_exprs,\n            'hessians': self.hessian_exprs\n        }\n</code></pre>"},{"location":"api/symbolic/#MomentEmu.symbolic_momentemu.SymbolicMomentEmu.get_symbolic_expressions","title":"<code>get_symbolic_expressions()</code>","text":"<p>Get the symbolic expressions.</p> Source code in <code>src/MomentEmu/symbolic_momentemu.py</code> <pre><code>def get_symbolic_expressions(self):\n    \"\"\"Get the symbolic expressions.\"\"\"\n    return {\n        'expressions': self.expressions,\n        'gradients': self.gradient_exprs,\n        'hessians': self.hessian_exprs\n    }\n</code></pre>"},{"location":"api/symbolic/#MomentEmu.symbolic_momentemu.SymbolicMomentEmu.gradient","title":"<code>gradient(X)</code>","text":"<p>Compute gradient at X.</p> Source code in <code>src/MomentEmu/symbolic_momentemu.py</code> <pre><code>def gradient(self, X):\n    \"\"\"Compute gradient at X.\"\"\"\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n\n    gradients = []\n    for x in X:\n        grad_at_x = []\n        for output_idx, grad_funcs in enumerate(self.gradient_funcs):\n            grad_output = np.array([func(*x) for func in grad_funcs])\n            grad_at_x.append(grad_output)\n        gradients.append(np.array(grad_at_x))\n\n    return np.array(gradients)\n</code></pre>"},{"location":"api/symbolic/#MomentEmu.symbolic_momentemu.SymbolicMomentEmu.hessian","title":"<code>hessian(X)</code>","text":"<p>Compute Hessian at X.</p> Source code in <code>src/MomentEmu/symbolic_momentemu.py</code> <pre><code>def hessian(self, X):\n    \"\"\"Compute Hessian at X.\"\"\"\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n\n    hessians = []\n    for x in X:\n        hess_at_x = []\n        for output_idx, hess_funcs in enumerate(self.hessian_funcs):\n            hess_matrix = np.array([[func(*x) for func in row] for row in hess_funcs])\n            hess_at_x.append(hess_matrix)\n        hessians.append(np.array(hess_at_x))\n\n    return np.array(hessians)\n</code></pre>"},{"location":"api/symbolic/#MomentEmu.symbolic_momentemu.SymbolicMomentEmu.predict","title":"<code>predict(X)</code>","text":"<p>Evaluate the emulator at X.</p> Source code in <code>src/MomentEmu/symbolic_momentemu.py</code> <pre><code>def predict(self, X):\n    \"\"\"Evaluate the emulator at X.\"\"\"\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n\n    predictions = []\n    for i, func in enumerate(self.lambdified):\n        pred = np.array([func(*x) for x in X])\n        predictions.append(pred)\n\n    return np.column_stack(predictions)\n</code></pre>"},{"location":"api/symbolic/#MomentEmu.symbolic_momentemu.create_symbolic_emulator","title":"<code>create_symbolic_emulator(trained_emulator, variable_names=None)</code>","text":"<p>Create a symbolic emulator from a trained MomentEmu.</p> Source code in <code>src/MomentEmu/symbolic_momentemu.py</code> <pre><code>def create_symbolic_emulator(trained_emulator, variable_names=None):\n    \"\"\"Create a symbolic emulator from a trained MomentEmu.\"\"\"\n    symbolic_emu = SymbolicMomentEmu(trained_emulator, variable_names)\n\n    # Return a dict with all the important components for easy access\n    return {\n        \"emulator\": symbolic_emu,\n        \"expression\": symbolic_emu.expressions[0] if symbolic_emu.expressions else None,\n        \"variables\": symbolic_emu.variables,\n        \"lambdified\": symbolic_emu.lambdified[0] if symbolic_emu.lambdified else None,\n        \"gradient_func\": lambda *args: symbolic_emu.gradient(np.array([args]))[0],\n        \"hessian_func\": lambda *args: symbolic_emu.hessian(np.array([args]))[0]\n    }\n</code></pre>"},{"location":"api/symbolic/#MomentEmu.symbolic_momentemu.demo_symbolic_autodiff","title":"<code>demo_symbolic_autodiff()</code>","text":"<p>Demonstrate symbolic auto-differentiation.</p> Source code in <code>src/MomentEmu/symbolic_momentemu.py</code> <pre><code>def demo_symbolic_autodiff():\n    \"\"\"Demonstrate symbolic auto-differentiation.\"\"\"\n\n    # Train regular MomentEmu\n    print(\"Training MomentEmu...\")\n    np.random.seed(42)\n    X_train = np.random.uniform(-1, 1, (50, 2))\n    Y_train = (X_train[:, 0]**2 + 2*X_train[:, 0]*X_train[:, 1] + X_train[:, 1]**2).reshape(-1, 1)\n\n    emulator = PolyEmu(X_train, Y_train, forward=True, backward=False)\n\n    # Create symbolic version\n    print(\"Creating symbolic version...\")\n    sym_emu = SymbolicMomentEmu(emulator, ['x', 'y'])\n\n    # Test point\n    x_test = np.array([[0.5, 0.3]])\n\n    # Predictions\n    y_pred = sym_emu.predict(x_test)\n    print(f\"Prediction: {y_pred[0]}\")\n\n    # Exact gradients\n    grad = sym_emu.gradient(x_test)\n    print(f\"Gradient: {grad[0]}\")\n\n    # Exact Hessians\n    hess = sym_emu.hessian(x_test)\n    print(f\"Hessian: {hess[0]}\")\n\n    # Show symbolic expressions\n    expressions = sym_emu.get_symbolic_expressions()\n    print(f\"\\\\nSymbolic expression: {expressions['expressions'][0]}\")\n    print(f\"Symbolic gradient: {expressions['gradients'][0]}\")\n</code></pre>"},{"location":"api/torch/","title":"PyTorch Integration API","text":"<p>PyTorch-based auto-differentiable MomentEmu implementation</p> <p>This module provides PyTorch integration for MomentEmu, enabling: - Native PyTorch nn.Module integration - Automatic gradient computation via autograd - GPU acceleration with CUDA support - Seamless ML pipeline integration</p> <p>Key components: - TorchMomentEmu: PyTorch nn.Module wrapper for MomentEmu - create_torch_emulator(): Convert trained MomentEmu to PyTorch format - demo_torch_autodiff(): Demonstration of PyTorch auto-differentiation capabilities</p>"},{"location":"api/torch/#MomentEmu.torch_momentemu.TorchMomentEmu","title":"<code>TorchMomentEmu</code>","text":"<p>               Bases: <code>Module</code></p> <p>PyTorch module for differentiable MomentEmu.</p> Source code in <code>src/MomentEmu/torch_momentemu.py</code> <pre><code>class TorchMomentEmu(nn.Module):\n    \"\"\"PyTorch module for differentiable MomentEmu.\"\"\"\n\n    def __init__(self, trained_emulator):\n        super().__init__()\n\n        # Convert coefficients to PyTorch parameters\n        self.coeffs = nn.Parameter(\n            torch.tensor(trained_emulator.forward_coeffs, dtype=torch.float32),\n            requires_grad=False  # Coefficients are fixed after training\n        )\n\n        # Store multi-indices and scaling parameters\n        self.multi_indices = trained_emulator.forward_multi_indices\n        self.register_buffer('input_mean', \n                           torch.tensor(trained_emulator.scaler_X.mean_, dtype=torch.float32))\n        self.register_buffer('input_scale', \n                           torch.tensor(trained_emulator.scaler_X.scale_, dtype=torch.float32))\n        self.register_buffer('output_mean', \n                           torch.tensor(trained_emulator.scaler_Y.mean_, dtype=torch.float32))\n        self.register_buffer('output_scale', \n                           torch.tensor(trained_emulator.scaler_Y.scale_, dtype=torch.float32))\n\n    def evaluate_monomials(self, X_scaled):\n        \"\"\"Evaluate monomials using PyTorch operations.\"\"\"\n        if X_scaled.dim() == 1:\n            X_scaled = X_scaled.unsqueeze(0)\n\n        batch_size, n_features = X_scaled.shape\n        n_terms = len(self.multi_indices)\n\n        # Initialize Phi matrix\n        Phi = torch.ones(batch_size, n_terms, device=X_scaled.device, dtype=X_scaled.dtype)\n\n        for j, alpha in enumerate(self.multi_indices):\n            monomial = torch.ones(batch_size, device=X_scaled.device, dtype=X_scaled.dtype)\n            for i, deg in enumerate(alpha):\n                if deg &gt; 0:\n                    monomial = monomial * (X_scaled[:, i] ** deg)\n            Phi[:, j] = monomial\n\n        return Phi\n\n    def forward(self, X):\n        \"\"\"Forward pass through the emulator.\"\"\"\n        if X.dim() == 1:\n            X = X.unsqueeze(0)\n\n        # Scale inputs\n        X_scaled = (X - self.input_mean) / self.input_scale\n\n        # Evaluate polynomials\n        Phi = self.evaluate_monomials(X_scaled)\n\n        # Predict scaled outputs\n        Y_scaled = Phi @ self.coeffs\n\n        # Unscale outputs\n        Y = Y_scaled * self.output_scale + self.output_mean\n\n        return Y\n</code></pre>"},{"location":"api/torch/#MomentEmu.torch_momentemu.TorchMomentEmu.evaluate_monomials","title":"<code>evaluate_monomials(X_scaled)</code>","text":"<p>Evaluate monomials using PyTorch operations.</p> Source code in <code>src/MomentEmu/torch_momentemu.py</code> <pre><code>def evaluate_monomials(self, X_scaled):\n    \"\"\"Evaluate monomials using PyTorch operations.\"\"\"\n    if X_scaled.dim() == 1:\n        X_scaled = X_scaled.unsqueeze(0)\n\n    batch_size, n_features = X_scaled.shape\n    n_terms = len(self.multi_indices)\n\n    # Initialize Phi matrix\n    Phi = torch.ones(batch_size, n_terms, device=X_scaled.device, dtype=X_scaled.dtype)\n\n    for j, alpha in enumerate(self.multi_indices):\n        monomial = torch.ones(batch_size, device=X_scaled.device, dtype=X_scaled.dtype)\n        for i, deg in enumerate(alpha):\n            if deg &gt; 0:\n                monomial = monomial * (X_scaled[:, i] ** deg)\n        Phi[:, j] = monomial\n\n    return Phi\n</code></pre>"},{"location":"api/torch/#MomentEmu.torch_momentemu.TorchMomentEmu.forward","title":"<code>forward(X)</code>","text":"<p>Forward pass through the emulator.</p> Source code in <code>src/MomentEmu/torch_momentemu.py</code> <pre><code>def forward(self, X):\n    \"\"\"Forward pass through the emulator.\"\"\"\n    if X.dim() == 1:\n        X = X.unsqueeze(0)\n\n    # Scale inputs\n    X_scaled = (X - self.input_mean) / self.input_scale\n\n    # Evaluate polynomials\n    Phi = self.evaluate_monomials(X_scaled)\n\n    # Predict scaled outputs\n    Y_scaled = Phi @ self.coeffs\n\n    # Unscale outputs\n    Y = Y_scaled * self.output_scale + self.output_mean\n\n    return Y\n</code></pre>"},{"location":"api/torch/#MomentEmu.torch_momentemu.create_torch_emulator","title":"<code>create_torch_emulator(trained_emulator)</code>","text":"<p>Create a PyTorch emulator from a trained MomentEmu.</p> Source code in <code>src/MomentEmu/torch_momentemu.py</code> <pre><code>def create_torch_emulator(trained_emulator):\n    \"\"\"Create a PyTorch emulator from a trained MomentEmu.\"\"\"\n    return TorchMomentEmu(trained_emulator)\n</code></pre>"},{"location":"api/torch/#MomentEmu.torch_momentemu.demo_torch_autodiff","title":"<code>demo_torch_autodiff()</code>","text":"<p>Demonstrate auto-differentiation with PyTorch.</p> Source code in <code>src/MomentEmu/torch_momentemu.py</code> <pre><code>def demo_torch_autodiff():\n    \"\"\"Demonstrate auto-differentiation with PyTorch.\"\"\"\n\n    # Train regular MomentEmu\n    print(\"Training MomentEmu...\")\n    np.random.seed(42)\n    X_train = np.random.uniform(-1, 1, (100, 3))\n    Y_train = (X_train[:, 0]**2 + X_train[:, 1]*X_train[:, 2]).reshape(-1, 1)\n\n    emulator = PolyEmu(X_train, Y_train, forward=True, backward=False)\n\n    # Convert to PyTorch\n    print(\"Converting to PyTorch...\")\n    torch_emu = TorchMomentEmu(emulator)\n\n    # Test point (requires gradient)\n    x_test = torch.tensor([0.5, 0.3, 0.2], requires_grad=True, dtype=torch.float32)\n\n    # Forward pass\n    y_pred = torch_emu(x_test)\n    print(f\"Prediction: {y_pred.item()}\")\n\n    # Compute gradient via backpropagation\n    y_pred.backward()\n    print(f\"Gradient: {x_test.grad}\")\n\n    # Compute Jacobian for multiple outputs (if needed)\n    x_test2 = torch.tensor([0.5, 0.3, 0.2], requires_grad=True, dtype=torch.float32)\n    y_pred2 = torch_emu(x_test2)\n\n    # Use autograd to compute Jacobian\n    jacobian = torch.autograd.functional.jacobian(torch_emu, x_test2)\n    print(f\"Jacobian shape: {jacobian.shape}\")\n    print(f\"Jacobian: {jacobian}\")\n</code></pre>"},{"location":"contributing/docs-setup/","title":"GitHub Actions for Documentation","text":"<p>Create a workflow to automatically build and deploy your documentation to GitHub Pages.</p> <p>Create this file: <code>.github/workflows/docs.yml</code></p> <pre><code>name: Build and Deploy Documentation\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\npermissions:\n  contents: read\n  pages: write\n  id-token: write\n\nconcurrency:\n  group: \"pages\"\n  cancel-in-progress: false\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n\n    steps:\n    - name: Checkout\n      uses: actions/checkout@v4\n\n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.10'\n\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install mkdocs mkdocs-material mkdocstrings[python] pymdown-extensions\n        pip install -e .\n\n    - name: Build documentation\n      run: mkdocs build --strict --verbose\n\n    - name: Setup Pages\n      if: github.ref == 'refs/heads/main'\n      uses: actions/configure-pages@v3\n\n    - name: Upload artifact\n      if: github.ref == 'refs/heads/main'\n      uses: actions/upload-pages-artifact@v2\n      with:\n        path: ./site\n\n  deploy:\n    if: github.ref == 'refs/heads/main'\n    environment:\n      name: github-pages\n      url: ${{ steps.deployment.outputs.page_url }}\n    runs-on: ubuntu-latest\n    needs: build\n\n    steps:\n    - name: Deploy to GitHub Pages\n      id: deployment\n      uses: actions/deploy-pages@v2\n</code></pre>"},{"location":"examples/use-cases/","title":"Use Cases and Applications","text":"<p>Explore real-world applications where MomentEmu excels.</p>"},{"location":"examples/use-cases/#scientific-computing","title":"Scientific Computing","text":""},{"location":"examples/use-cases/#cosmological-parameter-estimation","title":"Cosmological Parameter Estimation","text":"<p>PolyCAMB Example: Emulating the relationship between cosmological parameters and CMB power spectra.</p> <pre><code># Example: 6-parameter \u039bCDM model \u2192 CMB power spectrum\ncosmological_params = ['\u03a9\u2098', '\u03a9\u1d66', 'h', 'n\u209b', 'ln(10\u00b9\u2070A\u209b)', '\u03c4']\ncmb_spectrum = f_CAMB(cosmological_params)  # Expensive CAMB computation\n\n# Train bidirectional emulator\nemulator = PolyEmu(cosmological_params, cmb_spectrum, \n                   forward=True, backward=True)\n\n# Fast parameter estimation from observations\nobserved_spectrum = load_planck_data()\nestimated_params = emulator.backward_emulator(observed_spectrum)\n</code></pre> <p>Benefits: - \u26a1 1000\u00d7 faster than CAMB - \ud83c\udfaf Maintains cosmological accuracy - \ud83d\udd04 Bidirectional parameter-observable mapping</p>"},{"location":"examples/use-cases/#physics-simulations","title":"Physics Simulations","text":"<p>Accelerate expensive simulations in particle physics, fluid dynamics, climate modeling:</p> <pre><code># Replace expensive PDE solver with polynomial emulator\nsimulation_params = get_simulation_grid()  # Temperature, pressure, etc.\nsimulation_results = run_expensive_pde_solver(simulation_params)\n\nemulator = PolyEmu(simulation_params, simulation_results, forward=True)\n\n# Real-time parameter sweeps\nnew_params = optimize_experimental_conditions()\npredicted_results = emulator.forward_emulator(new_params)\n</code></pre>"},{"location":"examples/use-cases/#machine-learning-integration","title":"Machine Learning Integration","text":""},{"location":"examples/use-cases/#neural-network-components","title":"Neural Network Components","text":"<p>Hybrid architectures combining interpretable polynomials with neural networks:</p> <pre><code>import torch.nn as nn\nfrom MomentEmu.torch_momentemu import create_torch_emulator\n\nclass HybridPhysicsNN(nn.Module):\n    def __init__(self, physics_emulator):\n        super().__init__()\n        # Physics-informed component (interpretable)\n        self.physics = create_torch_emulator(physics_emulator)\n        # Data-driven component (flexible)\n        self.neural_net = nn.Sequential(\n            nn.Linear(10, 50),\n            nn.ReLU(),\n            nn.Linear(50, 10)\n        )\n\n    def forward(self, x):\n        physics_pred = self.physics(x)\n        residual = self.neural_net(x)\n        return physics_pred + residual\n</code></pre>"},{"location":"examples/use-cases/#transfer-learning","title":"Transfer Learning","text":"<p>Pre-trained emulators as starting points for new problems:</p> <pre><code># Pre-trained on large simulation dataset\nbase_emulator = PolyEmu(large_dataset_X, large_dataset_Y, forward=True)\n\n# Fine-tune on smaller, specific dataset\ncoeffs_base = base_emulator.coeffs_forward\n# Use as initialization for new problem...\n</code></pre>"},{"location":"examples/use-cases/#optimization-and-inference","title":"Optimization and Inference","text":""},{"location":"examples/use-cases/#bayesian-parameter-estimation","title":"Bayesian Parameter Estimation","text":"<p>MCMC sampling with fast likelihood evaluation:</p> <pre><code>import emcee\n\ndef log_likelihood(params):\n    predicted_obs = emulator.forward_emulator(params.reshape(1, -1))\n    return -0.5 * np.sum((predicted_obs - observed_data)**2 / sigma**2)\n\n# Fast MCMC sampling\nsampler = emcee.EnsembleSampler(nwalkers, ndim, log_likelihood)\nsampler.run_mcmc(initial_guess, nsteps)\n</code></pre>"},{"location":"examples/use-cases/#gradient-based-optimization","title":"Gradient-Based Optimization","text":"<p>Auto-differentiation for efficient optimization:</p> <pre><code># JAX optimization\nfrom jax import grad\nfrom jax.scipy.optimize import minimize\n\njax_emu = create_jax_emulator(emulator)\nobjective = lambda x: jnp.sum((jax_emu(x) - target)**2)\ngradient_fn = grad(objective)\n\nresult = minimize(objective, x0, jac=gradient_fn, method='BFGS')\n</code></pre>"},{"location":"examples/use-cases/#sensitivity-analysis","title":"Sensitivity Analysis","text":""},{"location":"examples/use-cases/#parameter-importance","title":"Parameter Importance","text":"<p>Analyze parameter sensitivity using symbolic derivatives:</p> <pre><code># Get symbolic expressions\nsymbolic_emu = create_symbolic_emulator(emulator, ['x1', 'x2', 'x3'])\nexpressions = symbolic_emu['expression']\ngradients = symbolic_emu['gradient']\n\n# Evaluate sensitivity at different points\nsensitivity_map = {}\nfor param_name, grad_expr in zip(['x1', 'x2', 'x3'], gradients):\n    sensitivity_map[param_name] = grad_expr.subs([(x1, 0.5), (x2, 0.3), (x3, 0.7)])\n</code></pre>"},{"location":"examples/use-cases/#uncertainty-propagation","title":"Uncertainty Propagation","text":"<p>Propagate parameter uncertainties through the emulator:</p> <pre><code># Monte Carlo uncertainty propagation\nparam_samples = np.random.multivariate_normal(mean_params, cov_params, 10000)\noutput_samples = emulator.forward_emulator(param_samples)\noutput_uncertainty = np.std(output_samples, axis=0)\n</code></pre>"},{"location":"examples/use-cases/#real-time-applications","title":"Real-Time Applications","text":""},{"location":"examples/use-cases/#control-systems","title":"Control Systems","text":"<p>Real-time system control with fast emulator responses:</p> <pre><code>class RealTimeController:\n    def __init__(self, system_emulator):\n        self.emulator = system_emulator\n\n    def control_update(self, current_state, target_state):\n        # Predict system response to different control inputs\n        control_candidates = generate_control_grid()\n        predicted_states = self.emulator.forward_emulator(control_candidates)\n\n        # Choose best control action\n        best_idx = np.argmin(np.linalg.norm(predicted_states - target_state, axis=1))\n        return control_candidates[best_idx]\n</code></pre>"},{"location":"examples/use-cases/#interactive-simulations","title":"Interactive Simulations","text":"<p>Real-time parameter exploration in scientific software:</p> <pre><code>import matplotlib.pyplot as plt\nfrom matplotlib.widgets import Slider\n\ndef interactive_explorer(emulator):\n    fig, ax = plt.subplots()\n\n    def update_plot(val):\n        params = [slider.val for slider in sliders]\n        result = emulator.forward_emulator(np.array(params).reshape(1, -1))\n        line.set_ydata(result.flatten())\n        fig.canvas.draw()\n\n    # Create sliders for each parameter\n    sliders = [Slider(ax_slider, f'Param {i}', 0, 1, valinit=0.5) \n               for i, ax_slider in enumerate(slider_axes)]\n\n    for slider in sliders:\n        slider.on_changed(update_plot)\n</code></pre>"},{"location":"examples/use-cases/#performance-benchmarks","title":"Performance Benchmarks","text":""},{"location":"examples/use-cases/#speed-comparisons","title":"Speed Comparisons","text":"Method Training Time Inference Time Use Case MomentEmu 0.1s 0.001s Fast approximation Gaussian Process 10s 0.1s Uncertainty quantification Neural Network 100s 0.01s Complex patterns Full Simulation 1000s 1000s Ground truth"},{"location":"examples/use-cases/#accuracy-benchmarks","title":"Accuracy Benchmarks","text":"<p>For typical scientific applications: - Forward emulation: 0.1-1% relative error - Inverse emulation: 1-5% parameter recovery error - Gradient accuracy: Near-exact (symbolic derivatives)</p>"},{"location":"examples/use-cases/#best-practices","title":"Best Practices","text":""},{"location":"examples/use-cases/#when-to-use-momentemu","title":"When to Use MomentEmu","text":"<p>\u2705 Ideal for: - Smooth, continuous mappings - Need for interpretability - Fast inference requirements - Limited training data - Gradient-based optimization</p> <p>\u274c Not suitable for: - Highly discontinuous functions - Very high-dimensional outputs (&gt;100) - Categorical/discrete outputs - Deep learning feature extraction</p>"},{"location":"examples/use-cases/#parameter-selection","title":"Parameter Selection","text":"<ul> <li>Polynomial degree: Start low (3-5), increase if needed</li> <li>Training data: 10-50\u00d7 more samples than polynomial terms</li> <li>Input scaling: Always standardize inputs</li> <li>Validation: Use held-out data for model selection</li> </ul>"},{"location":"examples/use-cases/#next-steps","title":"Next Steps","text":"<ul> <li>Explore Performance Benchmarks for detailed comparisons</li> <li>Check External Examples for PolyCAMB integration</li> <li>See Tutorials for step-by-step guides</li> </ul>"},{"location":"installation/autodiff/","title":"Auto-Differentiation Installation","text":"<p>Learn how to install MomentEmu with auto-differentiation support for different frameworks.</p>"},{"location":"installation/autodiff/#framework-overview","title":"Framework Overview","text":"<p>MomentEmu supports three auto-differentiation frameworks:</p> Framework Use Case Installation Command JAX High-performance computing, research <code>[jax]</code> PyTorch Machine learning, neural networks <code>[torch]</code> SymPy Symbolic analysis (included in core) Core installation"},{"location":"installation/autodiff/#jax-installation","title":"JAX Installation","text":"<p>Best for high-performance scientific computing and research applications.</p>"},{"location":"installation/autodiff/#standard-installation","title":"Standard Installation","text":"<pre><code>pip install \"git+https://github.com/zzhang0123/MomentEmu.git[jax]\"\n</code></pre>"},{"location":"installation/autodiff/#platform-specific-jax","title":"Platform-Specific JAX","text":"<p>For optimal performance, install platform-specific JAX:</p> CPU OnlyCUDA (GPU)TPU <pre><code>pip install \"jax[cpu]\"\npip install \"git+https://github.com/zzhang0123/MomentEmu.git\"\n</code></pre> <pre><code>pip install \"jax[cuda]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\npip install \"git+https://github.com/zzhang0123/MomentEmu.git\"\n</code></pre> <pre><code>pip install \"jax[tpu]\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\npip install \"git+https://github.com/zzhang0123/MomentEmu.git\"\n</code></pre>"},{"location":"installation/autodiff/#verify-jax-installation","title":"Verify JAX Installation","text":"<pre><code>import jax\nimport jax.numpy as jnp\nfrom MomentEmu import jax_momentemu\n\nprint(f\"JAX version: {jax.__version__}\")\nprint(f\"JAX devices: {jax.devices()}\")\nprint(\"\u2705 JAX integration ready!\")\n</code></pre>"},{"location":"installation/autodiff/#pytorch-installation","title":"PyTorch Installation","text":"<p>Best for machine learning applications and neural network integration.</p>"},{"location":"installation/autodiff/#standard-installation_1","title":"Standard Installation","text":"<pre><code>pip install \"git+https://github.com/zzhang0123/MomentEmu.git[torch]\"\n</code></pre>"},{"location":"installation/autodiff/#platform-specific-pytorch","title":"Platform-Specific PyTorch","text":"<p>For GPU support, visit pytorch.org for platform-specific instructions:</p> CPU OnlyCUDA 11.8CUDA 12.1 <pre><code>pip install torch --index-url https://download.pytorch.org/whl/cpu\npip install \"git+https://github.com/zzhang0123/MomentEmu.git\"\n</code></pre> <pre><code>pip install torch --index-url https://download.pytorch.org/whl/cu118\npip install \"git+https://github.com/zzhang0123/MomentEmu.git\"\n</code></pre> <pre><code>pip install torch --index-url https://download.pytorch.org/whl/cu121\npip install \"git+https://github.com/zzhang0123/MomentEmu.git\"\n</code></pre>"},{"location":"installation/autodiff/#verify-pytorch-installation","title":"Verify PyTorch Installation","text":"<pre><code>import torch\nfrom MomentEmu import torch_momentemu\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU device: {torch.cuda.get_device_name()}\")\nprint(\"\u2705 PyTorch integration ready!\")\n</code></pre>"},{"location":"installation/autodiff/#all-frameworks","title":"All Frameworks","text":"<p>Install everything at once:</p> <pre><code># All auto-differentiation frameworks\npip install \"git+https://github.com/zzhang0123/MomentEmu.git[autodiff]\"\n\n# Everything including visualization tools\npip install \"git+https://github.com/zzhang0123/MomentEmu.git[all]\"\n</code></pre>"},{"location":"installation/autodiff/#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/autodiff/#common-issues","title":"Common Issues","text":"<p>JAX Installation Issues</p> <ul> <li>M1/M2 Mac: Use <code>pip install jax[cpu]</code> for compatibility</li> <li>CUDA version mismatch: Ensure CUDA toolkit matches JAX requirements</li> <li>Old jaxlib: Update with <code>pip install --upgrade jax jaxlib</code></li> </ul> <p>PyTorch Installation Issues</p> <ul> <li>Import errors: Check PyTorch installation with <code>python -c \"import torch; print(torch.__version__)\"</code></li> <li>CUDA issues: Verify CUDA installation and version compatibility</li> <li>Memory errors: Start with CPU version for testing</li> </ul> <p>General Issues</p> <ul> <li>Dependency conflicts: Use a fresh virtual environment</li> <li>Import errors: Ensure you've installed MomentEmu after the framework</li> <li>Version mismatches: Use the latest versions of all packages</li> </ul>"},{"location":"installation/autodiff/#testing-your-installation","title":"Testing Your Installation","text":"<p>Run this comprehensive test:</p> <pre><code># Test all available frameworks\nimport numpy as np\nfrom MomentEmu import PolyEmu\n\n# Create test data\nX = np.random.rand(50, 2)\nY = (X[:, 0]**2 + X[:, 1]**2).reshape(-1, 1)\nemulator = PolyEmu(X, Y, forward=True)\n\nprint(\"Testing auto-differentiation frameworks:\")\n\n# Test JAX\ntry:\n    from MomentEmu import jax_momentemu\n    jax_emu = jax_momentemu.create_jax_emulator(emulator)\n    print(\"\u2705 JAX: Working\")\nexcept ImportError:\n    print(\"\u274c JAX: Not installed\")\nexcept Exception as e:\n    print(f\"\u274c JAX: Error - {e}\")\n\n# Test PyTorch\ntry:\n    from MomentEmu import torch_momentemu\n    torch_emu = torch_momentemu.create_torch_emulator(emulator)\n    print(\"\u2705 PyTorch: Working\")\nexcept ImportError:\n    print(\"\u274c PyTorch: Not installed\")\nexcept Exception as e:\n    print(f\"\u274c PyTorch: Error - {e}\")\n\n# Test SymPy (should always work)\ntry:\n    from MomentEmu import symbolic_momentemu\n    sym_emu = symbolic_momentemu.create_symbolic_emulator(emulator, ['x', 'y'])\n    print(\"\u2705 SymPy: Working\")\nexcept Exception as e:\n    print(f\"\u274c SymPy: Error - {e}\")\n</code></pre>"},{"location":"installation/autodiff/#next-steps","title":"Next Steps","text":"<ul> <li>Continue to Getting Started Tutorial</li> <li>Explore Auto-Differentiation Guide</li> <li>See Development Installation for contributors</li> </ul>"},{"location":"installation/development/","title":"Development Installation","text":"<p>Set up MomentEmu for development and contribution.</p>"},{"location":"installation/development/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.7 or higher</li> <li>Git</li> <li>Virtual environment tool (venv, conda, etc.)</li> </ul>"},{"location":"installation/development/#development-setup","title":"Development Setup","text":""},{"location":"installation/development/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/zzhang0123/MomentEmu.git\ncd MomentEmu\n</code></pre>"},{"location":"installation/development/#2-create-virtual-environment","title":"2. Create Virtual Environment","text":"venvconda <pre><code>python -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n</code></pre> <pre><code>conda create -n momentemu python=3.10\nconda activate momentemu\n</code></pre>"},{"location":"installation/development/#3-install-in-development-mode","title":"3. Install in Development Mode","text":"<pre><code># Install with all development dependencies\npip install -e \".[all]\"\n\n# Additional development tools\npip install pytest pytest-cov black isort flake8 mypy\n</code></pre>"},{"location":"installation/development/#development-dependencies","title":"Development Dependencies","text":"<p>The <code>[all]</code> installation includes:</p> <ul> <li>Core dependencies: numpy, scipy, sympy, scikit-learn</li> <li>Auto-diff frameworks: jax, jaxlib, torch</li> <li>Visualization: matplotlib</li> <li>Testing: pytest, pytest-cov (install separately)</li> <li>Code quality: black, isort, flake8, mypy (install separately)</li> </ul>"},{"location":"installation/development/#project-structure","title":"Project Structure","text":"<pre><code>MomentEmu/\n\u251c\u2500\u2500 src/MomentEmu/           # Main package\n\u2502   \u251c\u2500\u2500 __init__.py         # Package initialization\n\u2502   \u251c\u2500\u2500 MomentEmu.py        # Core emulator class\n\u2502   \u251c\u2500\u2500 jax_momentemu.py    # JAX integration\n\u2502   \u251c\u2500\u2500 torch_momentemu.py  # PyTorch integration\n\u2502   \u2514\u2500\u2500 symbolic_momentemu.py # SymPy integration\n\u251c\u2500\u2500 tests/                   # Test suite\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 test_autodiff_variants.py\n\u251c\u2500\u2500 docs/                    # Documentation\n\u251c\u2500\u2500 .github/workflows/       # CI/CD\n\u251c\u2500\u2500 pyproject.toml          # Package configuration\n\u251c\u2500\u2500 mkdocs.yml              # Documentation config\n\u2514\u2500\u2500 README.md               # Main documentation\n</code></pre>"},{"location":"installation/development/#running-tests","title":"Running Tests","text":""},{"location":"installation/development/#full-test-suite","title":"Full Test Suite","text":"<pre><code># Run all tests\npytest tests/ -v\n\n# Run with coverage\npytest tests/ --cov=src/MomentEmu --cov-report=html\n\n# Run specific test file\npytest tests/test_autodiff_variants.py -v\n</code></pre>"},{"location":"installation/development/#individual-module-tests","title":"Individual Module Tests","text":"<pre><code># Test core functionality\npython src/MomentEmu/MomentEmu.py\n\n# Test auto-diff modules (if dependencies installed)\npython src/MomentEmu/jax_momentemu.py\npython src/MomentEmu/torch_momentemu.py\npython src/MomentEmu/symbolic_momentemu.py\n</code></pre>"},{"location":"installation/development/#code-quality","title":"Code Quality","text":""},{"location":"installation/development/#formatting","title":"Formatting","text":"<pre><code># Format code with black\nblack src/ tests/\n\n# Sort imports with isort\nisort src/ tests/\n\n# Check formatting\nblack --check src/ tests/\nisort --check-only src/ tests/\n</code></pre>"},{"location":"installation/development/#linting","title":"Linting","text":"<pre><code># Check code style\nflake8 src/ tests/\n\n# Type checking\nmypy src/MomentEmu/\n</code></pre>"},{"location":"installation/development/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>Set up pre-commit hooks to ensure code quality:</p> <pre><code>pip install pre-commit\npre-commit install\n</code></pre> <p>Create <code>.pre-commit-config.yaml</code>:</p> <pre><code>repos:\n  - repo: https://github.com/psf/black\n    rev: 23.3.0\n    hooks:\n      - id: black\n  - repo: https://github.com/pycqa/isort\n    rev: 5.12.0\n    hooks:\n      - id: isort\n  - repo: https://github.com/pycqa/flake8\n    rev: 6.0.0\n    hooks:\n      - id: flake8\n</code></pre>"},{"location":"installation/development/#building-documentation","title":"Building Documentation","text":""},{"location":"installation/development/#local-development","title":"Local Development","text":"<pre><code># Install documentation dependencies\npip install mkdocs mkdocs-material mkdocstrings[python] pymdown-extensions\n\n# Serve documentation locally\nmkdocs serve\n\n# Build documentation\nmkdocs build\n</code></pre> <p>The documentation will be available at <code>http://localhost:8000</code>.</p>"},{"location":"installation/development/#documentation-structure","title":"Documentation Structure","text":"<ul> <li>docs/: Source files in Markdown</li> <li>mkdocs.yml: Configuration file</li> <li>site/: Generated HTML (git-ignored)</li> </ul>"},{"location":"installation/development/#contributing-workflow","title":"Contributing Workflow","text":""},{"location":"installation/development/#1-create-feature-branch","title":"1. Create Feature Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n</code></pre>"},{"location":"installation/development/#2-make-changes","title":"2. Make Changes","text":"<ul> <li>Write code following existing style</li> <li>Add tests for new functionality</li> <li>Update documentation as needed</li> <li>Ensure all tests pass</li> </ul>"},{"location":"installation/development/#3-quality-checks","title":"3. Quality Checks","text":"<pre><code># Run full test suite\npytest tests/ -v\n\n# Check code quality\nblack --check src/ tests/\nisort --check-only src/ tests/\nflake8 src/ tests/\n\n# Type checking\nmypy src/MomentEmu/\n</code></pre>"},{"location":"installation/development/#4-submit-pull-request","title":"4. Submit Pull Request","text":"<pre><code>git add .\ngit commit -m \"Description of changes\"\ngit push origin feature/your-feature-name\n</code></pre> <p>Then create a pull request on GitHub.</p>"},{"location":"installation/development/#release-process","title":"Release Process","text":""},{"location":"installation/development/#version-bumping","title":"Version Bumping","text":"<p>Update version in <code>pyproject.toml</code>:</p> <pre><code>[project]\nversion = \"1.1.0\"  # Update this\n</code></pre>"},{"location":"installation/development/#creating-releases","title":"Creating Releases","text":"<pre><code># Tag the release\ngit tag v1.1.0\ngit push origin v1.1.0\n\n# GitHub Actions will automatically:\n# - Run tests\n# - Build documentation\n# - Create release artifacts\n</code></pre>"},{"location":"installation/development/#troubleshooting-development-setup","title":"Troubleshooting Development Setup","text":""},{"location":"installation/development/#common-issues","title":"Common Issues","text":"<p>Import Errors</p> <ul> <li>Make sure you installed in development mode: <code>pip install -e .</code></li> <li>Check that your virtual environment is activated</li> <li>Verify Python path includes the project directory</li> </ul> <p>Test Failures</p> <ul> <li>Ensure all dependencies are installed: <code>pip install -e \".[all]\"</code></li> <li>Check Python version compatibility (3.7+)</li> <li>Run tests in isolation: <code>pytest tests/test_autodiff_variants.py::test_name -v</code></li> </ul> <p>Documentation Build Errors</p> <ul> <li>Install docs dependencies: <code>pip install mkdocs mkdocs-material mkdocstrings[python]</code></li> <li>Check for syntax errors in markdown files</li> <li>Verify <code>mkdocs.yml</code> configuration</li> </ul>"},{"location":"installation/development/#getting-help","title":"Getting Help","text":"<ul> <li>Check existing GitHub Issues</li> <li>Create new issue for bugs or feature requests</li> <li>Join discussions in GitHub Discussions</li> </ul>"},{"location":"installation/development/#next-steps","title":"Next Steps","text":"<ul> <li>Read Contributing Guidelines</li> <li>Explore Testing Framework</li> <li>Review Release Process</li> </ul>"},{"location":"installation/quick-start/","title":"Quick Start Installation","text":"<p>Get up and running with MomentEmu in minutes.</p>"},{"location":"installation/quick-start/#system-requirements","title":"System Requirements","text":"<ul> <li>Python 3.7 or higher</li> <li>NumPy, SciPy, SymPy, scikit-learn (automatically installed)</li> </ul>"},{"location":"installation/quick-start/#basic-installation","title":"Basic Installation","text":"<p>For core functionality only:</p> <pre><code>pip install git+https://github.com/zzhang0123/MomentEmu.git\n</code></pre> <p>This installs the lightweight version with all essential features for polynomial emulation.</p>"},{"location":"installation/quick-start/#installation-with-auto-differentiation","title":"Installation with Auto-Differentiation","text":"<p>Choose your preferred framework:</p> JAX (Recommended for HPC)PyTorch (ML Integration)All FrameworksEverything <p><pre><code>pip install \"git+https://github.com/zzhang0123/MomentEmu.git[jax]\"\n</code></pre> Best for high-performance computing, scientific research, and GPU acceleration.</p> <p><pre><code>pip install \"git+https://github.com/zzhang0123/MomentEmu.git[torch]\"\n</code></pre> Perfect for machine learning pipelines and neural network integration.</p> <p><pre><code>pip install \"git+https://github.com/zzhang0123/MomentEmu.git[autodiff]\"\n</code></pre> Installs both JAX and PyTorch support.</p> <p><pre><code>pip install \"git+https://github.com/zzhang0123/MomentEmu.git[all]\"\n</code></pre> Includes visualization tools (matplotlib) and all features.</p>"},{"location":"installation/quick-start/#verify-installation","title":"Verify Installation","text":"<p>Test your installation:</p> <pre><code>import numpy as np\nfrom MomentEmu import PolyEmu\n\n# Generate test data\nX = np.random.rand(100, 2)\nY = (X[:, 0]**2 + X[:, 1]**2).reshape(-1, 1)\n\n# Create emulator\nemulator = PolyEmu(X, Y, forward=True)\nprint(\"\u2705 MomentEmu installed successfully!\")\n\n# Test auto-differentiation (if installed)\ntry:\n    from MomentEmu import jax_momentemu\n    print(\"\u2705 JAX support available\")\nexcept ImportError:\n    print(\"\u26a0\ufe0f JAX not installed\")\n\ntry:\n    from MomentEmu import torch_momentemu\n    print(\"\u2705 PyTorch support available\")\nexcept ImportError:\n    print(\"\u26a0\ufe0f PyTorch not installed\")\n</code></pre>"},{"location":"installation/quick-start/#next-steps","title":"Next Steps","text":"<ul> <li>Continue to Getting Started Tutorial</li> <li>Learn about Auto-Differentiation Setup</li> <li>Explore Development Installation for contributors</li> </ul>"},{"location":"installation/quick-start/#troubleshooting","title":"Troubleshooting","text":"<p>Common Issues</p> <ul> <li>Import errors: Make sure you're using Python 3.7+</li> <li>JAX installation: On some systems, you may need platform-specific JAX installation</li> <li>PyTorch installation: Visit pytorch.org for platform-specific instructions</li> </ul> <p>Performance Tips</p> <ul> <li>Use JAX installation for fastest performance on large datasets</li> <li>The basic installation is sufficient for most use cases</li> <li>Consider GPU-enabled JAX for very large problems</li> </ul>"},{"location":"theory/mathematical-background/","title":"Mathematical Background","text":"<p>Understanding the mathematical foundations of MomentEmu's polynomial emulation approach.</p>"},{"location":"theory/mathematical-background/#core-concept","title":"Core Concept","text":"<p>MomentEmu builds polynomial emulators by solving a linear system derived from moment matrices, avoiding iterative optimization entirely.</p>"},{"location":"theory/mathematical-background/#the-moment-projection-method","title":"The Moment-Projection Method","text":""},{"location":"theory/mathematical-background/#problem-setup","title":"Problem Setup","text":"<p>Given training data \\(\\{(\\theta_i, y_i)\\}_{i=1}^N\\) where: - \\(\\theta_i \\in \\mathbb{R}^n\\) are input parameters - \\(y_i \\in \\mathbb{R}^m\\) are output observables</p> <p>We want to find a polynomial mapping \\(P: \\mathbb{R}^n \\to \\mathbb{R}^m\\) such that: \\(\\(P(\\theta_i) \\approx y_i \\quad \\text{for all } i\\)\\)</p>"},{"location":"theory/mathematical-background/#polynomial-basis","title":"Polynomial Basis","text":"<p>For a given maximum degree \\(d\\), we construct a polynomial basis using multi-indices: \\(\\(\\{\\theta^{\\alpha} : \\alpha \\in \\mathcal{A}_d\\}\\)\\)</p> <p>where \\(\\mathcal{A}_d = \\{\\alpha \\in \\mathbb{N}_0^n : |\\alpha| \\leq d\\}\\) and \\(\\theta^{\\alpha} = \\prod_{j=1}^n \\theta_j^{\\alpha_j}\\).</p> <p>Example: 2D quadratic basis</p> <p>For \\(n=2, d=2\\): \\(\\{1, \\theta_1, \\theta_2, \\theta_1^2, \\theta_1\\theta_2, \\theta_2^2\\}\\)</p>"},{"location":"theory/mathematical-background/#moment-matrix-construction","title":"Moment Matrix Construction","text":"<p>The moment matrix \\(M \\in \\mathbb{R}^{|\\mathcal{A}_d| \\times |\\mathcal{A}_d|}\\) is defined as: \\(\\(M_{\\alpha\\beta} = \\frac{1}{N} \\sum_{i=1}^N \\theta_i^{\\alpha} \\theta_i^{\\beta}\\)\\)</p> <p>This captures the correlations between all polynomial basis functions.</p>"},{"location":"theory/mathematical-background/#moment-vector-construction","title":"Moment Vector Construction","text":"<p>For each output dimension \\(j\\), the moment vector \\(\\nu^{(j)} \\in \\mathbb{R}^{|\\mathcal{A}_d|}\\) is: \\(\\(\\nu^{(j)}_{\\alpha} = \\frac{1}{N} \\sum_{i=1}^N \\theta_i^{\\alpha} y_i^{(j)}\\)\\)</p> <p>This represents the correlation between basis functions and output values.</p>"},{"location":"theory/mathematical-background/#linear-system-solution","title":"Linear System Solution","text":"<p>The polynomial coefficients \\(c^{(j)}\\) for output dimension \\(j\\) are found by solving: \\(\\(M c^{(j)} = \\nu^{(j)}\\)\\)</p> <p>This is a linear system - no iterative optimization required!</p>"},{"location":"theory/mathematical-background/#final-polynomial","title":"Final Polynomial","text":"<p>The resulting polynomial emulator is: \\(\\(P^{(j)}(\\theta) = \\sum_{\\alpha \\in \\mathcal{A}_d} c^{(j)}_{\\alpha} \\theta^{\\alpha}\\)\\)</p>"},{"location":"theory/mathematical-background/#key-advantages","title":"Key Advantages","text":""},{"location":"theory/mathematical-background/#1-non-iterative-training","title":"1. Non-Iterative Training","text":"<ul> <li>Traditional ML: Requires gradient descent, backpropagation, hyperparameter tuning</li> <li>MomentEmu: Direct linear algebra solution</li> </ul>"},{"location":"theory/mathematical-background/#2-interpretable-results","title":"2. Interpretable Results","text":"<ul> <li>Coefficients have clear mathematical meaning</li> <li>Symbolic expressions available</li> <li>Easy to analyze polynomial structure</li> </ul>"},{"location":"theory/mathematical-background/#3-fast-inference","title":"3. Fast Inference","text":"<ul> <li>Simple polynomial evaluation</li> <li>No complex model architecture</li> <li>Vectorizable operations</li> </ul>"},{"location":"theory/mathematical-background/#4-theoretical-guarantees","title":"4. Theoretical Guarantees","text":"<ul> <li>Well-conditioned for appropriate data scaling</li> <li>Unique solution (when \\(M\\) is invertible)</li> <li>Convergence properties from approximation theory</li> </ul>"},{"location":"theory/mathematical-background/#bidirectional-emulation","title":"Bidirectional Emulation","text":""},{"location":"theory/mathematical-background/#forward-mapping","title":"Forward Mapping","text":"<p>Standard case: \\(\\theta \\to y\\) \\(\\(y = P_{\\text{forward}}(\\theta)\\)\\)</p>"},{"location":"theory/mathematical-background/#inverse-mapping","title":"Inverse Mapping","text":"<p>For invertible relationships: \\(y \\to \\theta\\) \\(\\(\\theta = P_{\\text{backward}}(y)\\)\\)</p> <p>MomentEmu constructs separate moment matrices for each direction: - Forward: \\(M^{(\\theta)}_{\\alpha\\beta} = \\frac{1}{N} \\sum_i \\theta_i^{\\alpha} \\theta_i^{\\beta}\\) - Backward: \\(M^{(y)}_{\\alpha\\beta} = \\frac{1}{N} \\sum_i y_i^{\\alpha} y_i^{\\beta}\\)</p>"},{"location":"theory/mathematical-background/#computational-complexity","title":"Computational Complexity","text":""},{"location":"theory/mathematical-background/#training-time","title":"Training Time","text":"<ul> <li>Moment matrix: \\(O(N \\cdot |\\mathcal{A}_d|^2)\\)</li> <li>Linear solve: \\(O(|\\mathcal{A}_d|^3)\\)</li> <li>Total: \\(O(N \\cdot |\\mathcal{A}_d|^2 + |\\mathcal{A}_d|^3)\\)</li> </ul> <p>where \\(|\\mathcal{A}_d| = \\binom{n+d}{d}\\) (number of multi-indices).</p>"},{"location":"theory/mathematical-background/#inference-time","title":"Inference Time","text":"<ul> <li>Polynomial evaluation: \\(O(|\\mathcal{A}_d|)\\)</li> <li>Extremely fast for inference</li> </ul>"},{"location":"theory/mathematical-background/#numerical-considerations","title":"Numerical Considerations","text":""},{"location":"theory/mathematical-background/#conditioning","title":"Conditioning","text":"<ul> <li>Raw data: Can lead to ill-conditioned moment matrices</li> <li>Scaled data: Standardization improves conditioning significantly</li> <li>Regularization: Ridge regression for ill-conditioned cases</li> </ul>"},{"location":"theory/mathematical-background/#degree-selection","title":"Degree Selection","text":"<ul> <li>Trade-off: Higher degree \u2192 better fit but more coefficients</li> <li>Rule of thumb: \\(N \\geq 10 \\cdot |\\mathcal{A}_d|\\) (samples per coefficient)</li> <li>Validation: Use held-out data to select optimal degree</li> </ul>"},{"location":"theory/mathematical-background/#relationship-to-other-methods","title":"Relationship to Other Methods","text":""},{"location":"theory/mathematical-background/#vs-polynomial-regression","title":"vs. Polynomial Regression","text":"<ul> <li>Standard: Uses least-squares fitting</li> <li>MomentEmu: Uses moment-based approach with better numerical properties</li> </ul>"},{"location":"theory/mathematical-background/#vs-gaussian-processes","title":"vs. Gaussian Processes","text":"<ul> <li>GP: Probabilistic, kernel-based, scales as \\(O(N^3)\\)</li> <li>MomentEmu: Deterministic, polynomial, scales better with data size</li> </ul>"},{"location":"theory/mathematical-background/#vs-neural-networks","title":"vs. Neural Networks","text":"<ul> <li>NN: Black box, requires iterative training</li> <li>MomentEmu: Interpretable, direct solution</li> </ul>"},{"location":"theory/mathematical-background/#mathematical-references","title":"Mathematical References","text":"<ol> <li>Approximation Theory: Jackson's theorem, Weierstrass approximation</li> <li>Numerical Linear Algebra: Matrix conditioning, regularization</li> <li>Polynomial Interpolation: Vandermonde matrices, basis functions</li> <li>Moment Methods: Method of moments in statistics</li> </ol> <p>The mathematical foundation ensures that MomentEmu provides: - \u2705 Theoretical soundness - \u2705 Computational efficiency  - \u2705 Interpretable results - \u2705 Practical applicability</p>"},{"location":"tutorials/autodiff-guide/","title":"Auto-Differentiation Guide","text":"<p>MomentEmu supports automatic differentiation through three different frameworks, enabling gradient-based optimization, neural network integration, and exact symbolic analysis.</p>"},{"location":"tutorials/autodiff-guide/#overview","title":"Overview","text":"<p>Choose the framework that best fits your needs:</p> JAXPyTorchSymPy <p>High-Performance Computing</p> <ul> <li>JIT compilation for speed</li> <li>GPU acceleration  </li> <li>Automatic vectorization</li> <li>Best for: Research, large-scale computation</li> </ul> <p>Machine Learning Integration</p> <ul> <li>Native <code>nn.Module</code> integration</li> <li>Seamless ML pipelines</li> <li>CUDA support</li> <li>Best for: Neural networks, ML applications</li> </ul> <p>Symbolic Computation</p> <ul> <li>Exact symbolic differentiation</li> <li>Zero numerical error</li> <li>Arbitrary-order derivatives  </li> <li>Best for: Mathematical analysis, education</li> </ul>"},{"location":"tutorials/autodiff-guide/#performance-comparison","title":"Performance Comparison","text":"<p>Based on comprehensive testing:</p> Framework Forward Pass Gradient Computation Memory Usage JAX 0.046s (1000 samples) 0.109s (single) Low (batched) PyTorch 0.0002s (1000 samples) 0.019s (single) Medium SymPy 0.00001s (single) 0.00002s (single) Minimal"},{"location":"tutorials/autodiff-guide/#jax-implementation","title":"JAX Implementation","text":""},{"location":"tutorials/autodiff-guide/#installation","title":"Installation","text":"<pre><code>pip install \"git+https://github.com/zzhang0123/MomentEmu.git[jax]\"\n</code></pre>"},{"location":"tutorials/autodiff-guide/#basic-usage","title":"Basic Usage","text":"<pre><code>from MomentEmu import PolyEmu\nfrom MomentEmu.jax_momentemu import create_jax_emulator\nimport jax.numpy as jnp\nfrom jax import grad, jacfwd\n\n# Train regular MomentEmu\nemulator = PolyEmu(X_train, Y_train, forward=True)\n\n# Convert to JAX\njax_emu = create_jax_emulator(emulator)\n\n# Compute predictions and gradients\nx = jnp.array([0.5, 0.3])\ny = jax_emu(x)                                    # Forward pass\ngradient = grad(lambda x: jax_emu(x).sum())(x)   # Gradient\njacobian = jacfwd(jax_emu)(x)                     # Jacobian matrix\n</code></pre>"},{"location":"tutorials/autodiff-guide/#advanced-features","title":"Advanced Features","text":"<pre><code>from jax import vmap, jit\n\n# Vectorized computation over batch\nbatch_emu = vmap(jax_emu)\nX_batch = jnp.array([[0.5, 0.3], [0.1, 0.8], [0.7, 0.2]])\nY_batch = batch_emu(X_batch)\n\n# JIT compilation for speed\njit_emu = jit(jax_emu)\ny_fast = jit_emu(x)  # Compiled version\n\n# Higher-order derivatives\nhessian = jacfwd(grad(lambda x: jax_emu(x).sum()))(x)\n</code></pre>"},{"location":"tutorials/autodiff-guide/#pytorch-implementation","title":"PyTorch Implementation","text":""},{"location":"tutorials/autodiff-guide/#installation_1","title":"Installation","text":"<pre><code>pip install \"git+https://github.com/zzhang0123/MomentEmu.git[torch]\"\n</code></pre>"},{"location":"tutorials/autodiff-guide/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from MomentEmu import PolyEmu\nfrom MomentEmu.torch_momentemu import create_torch_emulator\nimport torch\n\n# Train regular MomentEmu\nemulator = PolyEmu(X_train, Y_train, forward=True)\n\n# Convert to PyTorch\ntorch_emu = create_torch_emulator(emulator)\n\n# Compute predictions and gradients\nx = torch.tensor([0.5, 0.3], requires_grad=True)\ny = torch_emu(x)\ny.backward()\ngradient = x.grad\n</code></pre>"},{"location":"tutorials/autodiff-guide/#neural-network-integration","title":"Neural Network Integration","text":"<pre><code>import torch.nn as nn\nimport torch.optim as optim\n\nclass HybridModel(nn.Module):\n    def __init__(self, moment_emu):\n        super().__init__()\n        self.moment_emu = create_torch_emulator(moment_emu)\n        self.linear = nn.Linear(1, 10)\n\n    def forward(self, x):\n        # Use MomentEmu as part of larger network\n        y = self.moment_emu(x)\n        return self.linear(y)\n\n# Training loop\nmodel = HybridModel(emulator)\noptimizer = optim.Adam(model.parameters())\nloss_fn = nn.MSELoss()\n\nfor epoch in range(100):\n    optimizer.zero_grad()\n    pred = model(X_train_torch)\n    loss = loss_fn(pred, Y_target)\n    loss.backward()\n    optimizer.step()\n</code></pre>"},{"location":"tutorials/autodiff-guide/#sympy-implementation","title":"SymPy Implementation","text":""},{"location":"tutorials/autodiff-guide/#basic-usage_2","title":"Basic Usage","text":"<pre><code>from MomentEmu import PolyEmu  \nfrom MomentEmu.symbolic_momentemu import create_symbolic_emulator\n\n# Train regular MomentEmu\nemulator = PolyEmu(X_train, Y_train, forward=True)\n\n# Convert to symbolic\nsym_dict = create_symbolic_emulator(emulator, variable_names=['x', 'y'])\n\n# Exact computations\nx_vals = [0.5, 0.3]\ny = sym_dict[\"lambdified\"](*x_vals)           # Numerical evaluation\ngradient = sym_dict[\"gradient_func\"](*x_vals)  # Exact gradient\nhessian = sym_dict[\"hessian_func\"](*x_vals)   # Exact Hessian\n</code></pre>"},{"location":"tutorials/autodiff-guide/#symbolic-analysis","title":"Symbolic Analysis","text":"<pre><code>import sympy as sp\n\n# Access symbolic expressions\nexpression = sym_dict[\"expression\"] \ngradient_expr = sym_dict[\"gradient\"]\nhessian_expr = sym_dict[\"hessian\"]\n\nprint(f\"Function: {expression}\")\nprint(f\"Gradient: {gradient_expr}\")\nprint(f\"Hessian: {hessian_expr}\")\n\n# Symbolic manipulation\nx, y = sp.symbols('x y')\nsimplified = sp.simplify(expression)\ntaylor_expansion = expression.series(x, 0, n=3)\n</code></pre>"},{"location":"tutorials/autodiff-guide/#choosing-the-right-framework","title":"Choosing the Right Framework","text":""},{"location":"tutorials/autodiff-guide/#use-jax-when","title":"Use JAX when:","text":"<ul> <li>You need maximum computational performance</li> <li>Working with large datasets or parameter sweeps</li> <li>Require GPU acceleration</li> <li>Building scientific computing applications</li> </ul>"},{"location":"tutorials/autodiff-guide/#use-pytorch-when","title":"Use PyTorch when:","text":"<ul> <li>Integrating with machine learning pipelines</li> <li>Building neural network architectures</li> <li>Need automatic differentiation in training loops</li> <li>Working with existing PyTorch models</li> </ul>"},{"location":"tutorials/autodiff-guide/#use-sympy-when","title":"Use SymPy when:","text":"<ul> <li>Performing mathematical analysis</li> <li>Need exact (not numerical) derivatives</li> <li>Want to understand the mathematical form</li> <li>Educational or research applications requiring symbolic expressions</li> </ul>"},{"location":"tutorials/autodiff-guide/#testing-and-validation","title":"Testing and Validation","text":"<p>All implementations are thoroughly tested:</p> <pre><code># Run comprehensive test suite\npython -m pytest tests/test_autodiff_variants.py -v\n\n# Individual framework tests\npython src/MomentEmu/jax_momentemu.py      # JAX tests\npython src/MomentEmu/torch_momentemu.py    # PyTorch tests  \npython src/MomentEmu/symbolic_momentemu.py # SymPy tests\n</code></pre> <p>The test results confirm: \u2705 Identical gradient magnitudes across all frameworks \u2705 Correct forward pass computations \u2705 Proper integration with each framework's ecosystem \u2705 100% test success rate</p>"},{"location":"tutorials/autodiff-guide/#next-steps","title":"Next Steps","text":"<ul> <li>Explore Use Cases for real-world applications</li> <li>See Performance Benchmarks for detailed comparisons</li> <li>Check API Reference for complete function documentation</li> </ul>"},{"location":"tutorials/getting-started/","title":"Getting Started with MomentEmu","text":"<p>This tutorial will walk you through the basic usage of MomentEmu for polynomial emulation.</p>"},{"location":"tutorials/getting-started/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>How to create and train a polynomial emulator</li> <li>Forward and inverse emulation</li> <li>Evaluating emulator performance</li> <li>Getting symbolic expressions</li> </ul>"},{"location":"tutorials/getting-started/#basic-example","title":"Basic Example","text":"<p>Let's start with a simple 2D function: \\(f(x, y) = x^2 + y^2\\)</p> <pre><code>import numpy as np\nfrom MomentEmu import PolyEmu\nimport matplotlib.pyplot as plt\n\n# Generate training data\nnp.random.seed(42)\nn_train = 100\nX_train = np.random.uniform(-1, 1, (n_train, 2))  # Parameters\nY_train = (X_train[:, 0]**2 + X_train[:, 1]**2).reshape(-1, 1)  # Observables\n\nprint(f\"Training data shape: X={X_train.shape}, Y={Y_train.shape}\")\n</code></pre>"},{"location":"tutorials/getting-started/#creating-an-emulator","title":"Creating an Emulator","text":"<pre><code># Create emulator with forward mapping\nemulator = PolyEmu(\n    X_train, Y_train,\n    forward=True,              # Enable forward emulation: \u03b8 \u2192 y\n    max_degree_forward=5,      # Maximum polynomial degree\n    backward=False,            # Disable inverse for this example\n    scaler_type='standardize'  # Standardize inputs for better conditioning\n)\n\nprint(\"\u2705 Emulator trained successfully!\")\nprint(f\"Forward degree: {emulator.degree_forward}\")\nprint(f\"Number of coefficients: {len(emulator.coeffs_forward)}\")\n</code></pre>"},{"location":"tutorials/getting-started/#making-predictions","title":"Making Predictions","text":"<pre><code># Generate test data\nn_test = 20\nX_test = np.random.uniform(-1, 1, (n_test, 2))\nY_true = (X_test[:, 0]**2 + X_test[:, 1]**2).reshape(-1, 1)\n\n# Forward prediction\nY_pred = emulator.forward_emulator(X_test)\n\n# Calculate error\nerror = np.abs(Y_pred - Y_true)\nprint(f\"Mean absolute error: {np.mean(error):.6f}\")\nprint(f\"Max absolute error: {np.max(error):.6f}\")\n</code></pre>"},{"location":"tutorials/getting-started/#visualizing-results","title":"Visualizing Results","text":"<pre><code>import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 4))\n\n# Plot 1: True vs Predicted\nplt.subplot(1, 2, 1)\nplt.scatter(Y_true, Y_pred, alpha=0.7)\nplt.plot([Y_true.min(), Y_true.max()], [Y_true.min(), Y_true.max()], 'r--', lw=2)\nplt.xlabel('True Values')\nplt.ylabel('Predicted Values')\nplt.title('True vs Predicted')\nplt.grid(True, alpha=0.3)\n\n# Plot 2: Error distribution\nplt.subplot(1, 2, 2)\nplt.hist(error.flatten(), bins=10, alpha=0.7, color='orange')\nplt.xlabel('Absolute Error')\nplt.ylabel('Frequency')\nplt.title('Error Distribution')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"tutorials/getting-started/#getting-symbolic-expressions","title":"Getting Symbolic Expressions","text":"<p>One of MomentEmu's key features is providing interpretable, symbolic polynomial expressions:</p> <pre><code># Get symbolic expressions\nexpressions = emulator.generate_forward_symb_emu()\n\nprint(\"Symbolic polynomial expression:\")\nprint(f\"f(x\u2080, x\u2081) = {expressions[0]}\")\n\n# The expression should be close to: x\u2080\u00b2 + x\u2081\u00b2\n</code></pre>"},{"location":"tutorials/getting-started/#bidirectional-emulation","title":"Bidirectional Emulation","text":"<p>For invertible mappings, MomentEmu can perform inverse emulation:</p> <pre><code># Create bidirectional emulator\nemulator_bidir = PolyEmu(\n    X_train, Y_train,\n    forward=True,\n    backward=True,              # Enable inverse emulation: y \u2192 \u03b8\n    max_degree_forward=5,\n    max_degree_backward=3       # Usually lower degree for inverse\n)\n\n# Forward: parameters \u2192 observables\nY_forward = emulator_bidir.forward_emulator(X_test[:5])\n\n# Inverse: observables \u2192 parameters  \nX_inverse = emulator_bidir.backward_emulator(Y_forward)\n\nprint(\"Inverse emulation test:\")\nprint(f\"Original parameters shape: {X_test[:5].shape}\")\nprint(f\"Recovered parameters shape: {X_inverse.shape}\")\nprint(f\"Recovery error: {np.mean(np.abs(X_test[:5] - X_inverse)):.6f}\")\n</code></pre>"},{"location":"tutorials/getting-started/#performance-tips","title":"Performance Tips","text":"<p>Optimization Guidelines</p> <ul> <li>Degree selection: Start with low degrees (3-5) and increase if needed</li> <li>Training data: Use 10-50\u00d7 more samples than the number of polynomial terms</li> <li>Scaling: Always use input scaling for better numerical conditioning</li> <li>Validation: Split your data to assess generalization performance</li> </ul>"},{"location":"tutorials/getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Forward Emulation in detail</li> <li>Explore Inverse Emulation capabilities  </li> <li>Try Auto-Differentiation for gradient-based applications</li> <li>See Use Cases for real-world applications</li> </ul>"},{"location":"tutorials/getting-started/#common-issues","title":"Common Issues","text":"<p>Troubleshooting</p> <ul> <li>Poor accuracy: Try increasing polynomial degree or adding more training data</li> <li>Numerical instability: Enable input scaling with <code>scaler_type='standardize'</code></li> <li>Slow training: Reduce polynomial degree or dataset size</li> <li>Inverse fails: Check that your mapping is actually invertible</li> </ul>"}]}